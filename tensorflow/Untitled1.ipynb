{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = tf.arg_max([[2,0,1],\n",
    "               [4,9,6],\n",
    "               [3,7,9]],dimension=1,name=None)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.add(x,y,name=None)\n",
    "\n",
    "x = tf.add_n([[1, 2],   ==> [4 6]\n",
    "              [3, 4]])\n",
    "\n",
    "# tensor 'a' is [[1, 2], [3, 4]]\n",
    "# tensor `b` is [[5, 0], [0, 6]]\n",
    "tf.accumulate_n([a, b, a]) ==> [[7, 4],\n",
    "                                [6, 14]]\n",
    "\n",
    "x = tf.argmax([[2,0,1],                 \n",
    "               [4,9,6],\n",
    "               [3,7,9]],axis=1,name=None)   ==> [0 1 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf_math\n",
    "tf.abs（x）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-2390619e9d53>:15: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0.55685\n",
      "0 [-0.39370239] [ 0.70380229]\n",
      "20 [-0.04767764] [ 0.37204927]\n",
      "40 [ 0.0609101] [ 0.31907126]\n",
      "60 [ 0.089653] [ 0.30504814]\n",
      "80 [ 0.09726118] [ 0.30133623]\n",
      "100 [ 0.09927504] [ 0.30035371]\n",
      "120 [ 0.0998081] [ 0.30009362]\n",
      "140 [ 0.0999492] [ 0.30002481]\n",
      "160 [ 0.09998656] [ 0.30000657]\n",
      "180 [ 0.09999644] [ 0.30000174]\n",
      "200 [ 0.09999905] [ 0.30000049]\n"
     ]
    }
   ],
   "source": [
    "# create data\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data*0.1 + 0.3\n",
    "\n",
    "### create tensorflow structure start ###\n",
    "Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "biases = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "y = Weights*x_data + biases\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y-y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "### create tensorflow structure end ###\n",
    "\n",
    "sess = tf.Session()\n",
    "# tf.initialize_all_variables() no long valid from\n",
    "# 2017-03-02 if using tensorflow >= 0.12\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print sess.run(loss)\n",
    "\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print step, sess.run(Weights), sess.run(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12]]\n",
      "[[12]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3,3]])\n",
    "matrix2 = tf.constant([[2],\n",
    "                       [2]])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "# method 1\n",
    "sess = tf.Session()\n",
    "result = sess.run(product)\n",
    "print result\n",
    "sess.close()\n",
    "\n",
    "# method 2\n",
    "with tf.Session() as sess:\n",
    "    result2 = sess.run(product)\n",
    "    print result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "state = tf.Variable(0, name='counter')\n",
    "# print state.name\n",
    "\n",
    "one = tf.constant(1)\n",
    "\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# init = tf.initialize_all_variables() # important\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for _ in range(3):\n",
    "        sess.run(update)\n",
    "        print sess.run(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14.]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "\n",
    "output = tf.multiply(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(output, feed_dict={input1:[7.], input2:[2.]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-243b893b867d>:33: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHoTCnxBCepD/DwSQUN6oR3lwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7efaa9ad10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "# Make up some real data\n",
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, None])\n",
    "ys = tf.placeholder(tf.float32, [None, None])\n",
    "# add hidden layer\n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)\n",
    "\n",
    "# the error between prediction and real data\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                     reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# important step\n",
    "# tf.initialize_all_variables() no long valid from\n",
    "# 2017-03-02 if using tensorflow >= 0.12\n",
    "if int((tf.__version__).split('.')[1]) < 12:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.scatter(x_data, y_data)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "for i in range(1000):\n",
    "    # training\n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "    if i % 50 == 0:\n",
    "        # to see the step improvement\n",
    "        # print sess.run(loss, feed_dict={xs: x_data, ys: y_data})\n",
    "        try:\n",
    "            ax.lines.remove(lines[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "        prediction_value = sess.run(prediction, feed_dict={xs:x_data})\n",
    "        lines = ax.plot(x_data, prediction_value, 'r-', lw=5)\n",
    "        \n",
    "        plt.pause(0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f801f96a550>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f8041fddd90>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f80421b5450>)\n",
      "0.098\n",
      "0.9507\n",
      "0.965\n",
      "0.9727\n",
      "0.9744\n",
      "0.9766\n",
      "0.9772\n",
      "0.9778\n",
      "0.978\n",
      "0.9791\n",
      "0.979\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# number 1 to 10 data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "print mnist\n",
    "\n",
    "############################3\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None,para=1):###\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(para*tf.random_uniform([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.01,)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    return result\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, 784])/255. # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# add output layer\n",
    "L1 = add_layer(xs, 784, 100, activation_function=tf.nn.tanh, para=0.01)\n",
    "L2 = add_layer(L1, 100, 100, activation_function=tf.nn.tanh, para=0.01)\n",
    "prediction = add_layer(L2, 100, 10,  activation_function=tf.nn.softmax)\n",
    "\n",
    "# the error between prediction and real data\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
    "                                              reduction_indices=[1]))       # loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "# important step\n",
    "# tf.initialize_all_variables() no long valid from\n",
    "# 2017-03-02 if using tensorflow >= 0.12\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10001):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "    if i % 1000 == 0:\n",
    "        print compute_accuracy(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f801c113f90>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f801c2d5ad0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f801c12a210>)\n",
      "0.2618\n",
      "0.9564\n",
      "0.9692\n",
      "0.9753\n",
      "0.9771\n",
      "0.977\n",
      "0.9775\n",
      "0.9767\n",
      "0.9788\n",
      "0.9789\n",
      "0.978\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# number 1 to 10 data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "print mnist\n",
    "###############################\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None,para=0.1):######\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(para*tf.random_normal([in_size, out_size]))########\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.01,)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    return result\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, 784])/255. # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# add output layer\n",
    "L1 = add_layer(xs, 784, 100, activation_function=tf.nn.tanh, para=0.01)\n",
    "L2 = add_layer(L1, 100, 100, activation_function=tf.nn.tanh, para=0.01)\n",
    "prediction = add_layer(L2, 100, 10,  activation_function=tf.nn.softmax)\n",
    "\n",
    "# the error between prediction and real data\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
    "                                              reduction_indices=[1]))       # loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "# important step\n",
    "# tf.initialize_all_variables() no long valid from\n",
    "# 2017-03-02 if using tensorflow >= 0.12\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10001):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "    if i % 1000 == 0:\n",
    "        print compute_accuracy(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.0685\n",
      "0.9681\n",
      "0.9794\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] = 1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, 784])/255.   # 28x28\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_image = tf.reshape(xs, [-1, 28, 28, 1])\n",
    "# print(x_image.shape)  # [n_samples, 28,28,1]\n",
    "\n",
    "## conv1 layer ##\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1)) # patch 5x5, in size 1, out size 32\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32\n",
    "h_pool1 = max_pool_2x2(h_conv1)                                         # output size 14x14x32\n",
    "\n",
    "## conv2 layer ##\n",
    "W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64\n",
    "h_pool2 = max_pool_2x2(h_conv2)                                         # output size 7x7x64\n",
    "\n",
    "## fc1 layer ##\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "## fc2 layer ##\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "# the error between prediction and real data\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))       # loss\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "# important step\n",
    "# tf.initialize_all_variables() no long valid from\n",
    "# 2017-03-02 if using tensorflow >= 0.12\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(2001):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})\n",
    "    if i % 1000 == 0:\n",
    "        print compute_accuracy(mnist.test.images, mnist.test.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.1688\n",
      "0.9739\n",
      "0.9779\n",
      "0.9832\n",
      "0.9869\n",
      "0.9888\n",
      "0.9861\n",
      "0.9867\n",
      "0.9888\n",
      "0.9878\n",
      "0.9885\n",
      "0.988\n",
      "0.9901\n",
      "0.9912\n",
      "0.9885\n",
      "0.9912\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None,784])\n",
    "ys = tf.placeholder(tf.float32, [None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_image = tf.reshape(xs, [-1,28,28,1])\n",
    "\n",
    "\n",
    "Weight_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1), dtype=tf.float32, name='conv_1_w')\n",
    "biases_conv1 = tf.Variable(tf.constant(0.1, shape=[32]), dtype=tf.float32, name=\"conv_1_b\")\n",
    "conv_1 = tf.nn.relu(tf.nn.conv2d(x_image, Weight_conv1, strides=[1,1,1,1], padding='SAME') + biases_conv1)\n",
    "pool_1 = tf.nn.max_pool(conv_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "\n",
    "Weight_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1), dtype=tf.float32, name=\"conv_2_w\")\n",
    "biases_conv2 = tf.Variable(tf.constant(0.1, shape=[64]),  dtype=tf.float32, name='conv_2_b')\n",
    "conv_2 = tf.nn.relu(tf.nn.conv2d(pool_1, Weight_conv2, strides=[1,1,1,1], padding=\"SAME\")  + biases_conv2)\n",
    "pool_2 = tf.nn.max_pool(conv_2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "\n",
    "pool_2_reshape = tf.reshape(pool_2, [-1, 7*7*64])\n",
    "Weight_fc1 = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1),dtype=tf.float32, name=\"Weight_fc1\")\n",
    "biases_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]), dtype=tf.float32, name=\"biases_fc1\")\n",
    "fc1 = tf.nn.relu(tf.matmul(pool_2_reshape, Weight_fc1) + biases_fc1)\n",
    "# drop_fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "Weight_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1), dtype=tf.float32, name='Weight_fc2')\n",
    "biases_fc2= tf.Variable(tf.constant(0.1, shape=[10]), dtype=tf.float32, name='biases_fc2')\n",
    "prediction = tf.nn.softmax(tf.matmul(fc1, Weight_fc2) + biases_fc2)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "train_imgs = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(15001):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})\n",
    "    if i % 1000 == 0:\n",
    "        print compute_accuracy(mnist.test.images, mnist.test.labels)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
